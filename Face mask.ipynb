{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c557721-5be5-4c13-b09c-7fa3a0b23bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face mask detection system\n",
    "#cnn-->it is used to process and recognize image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#we will perform zooming,rotation,shearing-->so that we can generate variety of img\n",
    "train_datagen=ImageDataGenerator(zoom_range=0.2,shear_range=0.2,rotation_range=0.2,rescale=1/255)  #ye training k liye noise bnayi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915232aa-1f38-4cc2-90f7-acfc683d03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1/255)  #no noise req here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec57a518-b89f-4955-9bd6-2466382edcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1314 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#next step is giving path to these generators(noise upr bnayi ab, daalni data me)\n",
    "train_dataset=train_datagen.flow_from_directory(r\"C:\\Users\\Micky\\Desktop\\face deection\\train\",batch_size=16,target_size=(150,150),class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081969db-668b-4104-a506-d5b84e63ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset=train_datagen.flow_from_directory(r\"C:\\Users\\Micky\\Desktop\\face deection\\test\",batch_size=16,target_size=(150,150),class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8cd1f2-83d4-4d46-9a0c-b50c0fcb13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making cnn structure now\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaca04b8-2f94-4fa4-948f-0a2891c10a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#con2d-->eyes store only particular part(only some feauters store of image)\n",
    "#convolution-->methods of extractung the features of image by convolving a filter over image to extract a part feature\n",
    "#filters in array form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83af2224-4a0e-4f62-891d-6d0678d44c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micky\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn=Sequential()  #seq model bna rhe\n",
    "cnn.add(Conv2D(input_shape=(150,150,3),filters=32,kernel_size=3,activation=\"relu\"))    #filtersize/kernelsize is always odd tabhi convolution hoga\n",
    "#conv2d is responsible to do convolution and generate multiple features..imput layer bn gyi\n",
    "#32 filters applied on image and 32 features generated of one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feaad861-df96-46e0-a9bd-6cfe7075b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we hav generated hige amt of data from img now we hav to perform reduction therefore Pooling\n",
    "#pooling types-->max and avg pooling\n",
    "cnn.add(MaxPool2D(pool_size=2,strides=2))   #layer bnai pooling k liye taki data km ho\n",
    "#strides is no of steps to skip for finding the values  max me max vale lega and avg pool me avg leag 2x2 array se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545af79d-d6f7-4bde-9b18-e714682ddeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e67b48a-9eed-405c-9299-636192d949bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make hidden layer\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(units=120,activation=\"relu\"))  # relu bcoz pixels amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28caa30b-642f-4b47-bd08-964a442afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make output layer\n",
    "cnn.add(Dense(units=1,activation=\"sigmoid\"))  #unit 1 bcoz mask h ya ni bas  sigmoid bcz 0 01 1 value chaiye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20bf7978-21e3-4f79-8233-7c19c67048fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile--  dedicing the strategy  how to train nn\n",
    "cnn.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7193f9-2d99-4f23-95d5-49b181146d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micky\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.5849 - loss: 2.9053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micky\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 329ms/step - accuracy: 0.5859 - loss: 2.8862 - val_accuracy: 0.8351 - val_loss: 0.4318\n",
      "Epoch 2/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 304ms/step - accuracy: 0.8858 - loss: 0.2974 - val_accuracy: 0.9278 - val_loss: 0.2001\n",
      "Epoch 3/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 299ms/step - accuracy: 0.9452 - loss: 0.1621 - val_accuracy: 0.9433 - val_loss: 0.1302\n",
      "Epoch 4/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 298ms/step - accuracy: 0.9699 - loss: 0.1002 - val_accuracy: 0.9433 - val_loss: 0.1338\n",
      "Epoch 5/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 303ms/step - accuracy: 0.9776 - loss: 0.0820 - val_accuracy: 0.9485 - val_loss: 0.1277\n",
      "Epoch 6/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 301ms/step - accuracy: 0.9690 - loss: 0.0883 - val_accuracy: 0.9485 - val_loss: 0.1317\n",
      "Epoch 7/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 301ms/step - accuracy: 0.9748 - loss: 0.0763 - val_accuracy: 0.9588 - val_loss: 0.1240\n",
      "Epoch 8/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 303ms/step - accuracy: 0.9842 - loss: 0.0649 - val_accuracy: 0.9485 - val_loss: 0.1315\n",
      "Epoch 9/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 302ms/step - accuracy: 0.9827 - loss: 0.0618 - val_accuracy: 0.9639 - val_loss: 0.1203\n",
      "Epoch 10/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 300ms/step - accuracy: 0.9830 - loss: 0.0636 - val_accuracy: 0.9485 - val_loss: 0.1444\n",
      "Epoch 11/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 306ms/step - accuracy: 0.9756 - loss: 0.0760 - val_accuracy: 0.9536 - val_loss: 0.1638\n",
      "Epoch 12/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 298ms/step - accuracy: 0.9710 - loss: 0.0673 - val_accuracy: 0.9588 - val_loss: 0.1433\n",
      "Epoch 13/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 306ms/step - accuracy: 0.9869 - loss: 0.0354 - val_accuracy: 0.9588 - val_loss: 0.1626\n",
      "Epoch 14/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 305ms/step - accuracy: 0.9916 - loss: 0.0337 - val_accuracy: 0.9588 - val_loss: 0.1484\n",
      "Epoch 15/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 302ms/step - accuracy: 0.9901 - loss: 0.0371 - val_accuracy: 0.9485 - val_loss: 0.1909\n",
      "Epoch 16/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 303ms/step - accuracy: 0.9902 - loss: 0.0327 - val_accuracy: 0.9381 - val_loss: 0.1588\n",
      "Epoch 17/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 303ms/step - accuracy: 0.9888 - loss: 0.0313 - val_accuracy: 0.9588 - val_loss: 0.1291\n",
      "Epoch 18/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 303ms/step - accuracy: 0.9975 - loss: 0.0125 - val_accuracy: 0.9433 - val_loss: 0.1747\n",
      "Epoch 19/50\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 302ms/step - accuracy: 0.9960 - loss: 0.0172 - val_accuracy: 0.9381 - val_loss: 0.2254\n",
      "Epoch 20/50\n",
      "\u001b[1m54/83\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 286ms/step - accuracy: 0.9933 - loss: 0.0160"
     ]
    }
   ],
   "source": [
    "#fit\n",
    "cnn.fit(train_dataset,validation_data=test_dataset,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea67ea-686c-44d9-8b49-1580ee7dec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"mymodel_1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a55325-02a0-4020-8bff-316d94ddf32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the directory\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\Micky\\Desktop\\face deection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397144d7-90ea-4923-8f08-e8e24e40657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfom the detection by live camera\n",
    "#importing libraries\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b50140-6fe7-47b7-937c-97e1f7e042e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load krna save wale ko\n",
    "cnn=load_model(\"mymodel_1.keras\")   #mymodel.h5 save da hua mam ne wahi daal dia abhi k liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5434c8-2413-4433-ba37-2dbdd0e14203",
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "#using the haar features for face detection\n",
    "#ab apke image me detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607d116-2d72-4ca3-8db4-5d7eab7dc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ab extract faces from frame/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e59574-4cef-4d53-9c93-4c8b3bf8753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#live video\n",
    "cap=cv2.VideoCapture(0) #camera apne pc ka open krne k liye\n",
    "while cap.isOpened():\n",
    "    b,img=cap.read()\n",
    "    cv2.imshow(\"frame\",img)\n",
    "    face=cascade.detectMultiScale(img)  #detecting faces using haar cascade.this func returns a list of rectangle where each rectangle represent a detected facein x,y,w,h\n",
    "    #loop over deetced face\n",
    "    #face detct to krlia mutlti scale ab extraction\n",
    "    for x,y,w,h in face:  #extraction\n",
    "        #extract the face regio slicing se\n",
    "        face=img[y:y+h,x:x+w]\n",
    "        #ab save krna\n",
    "        cv2.imwrite(\"face.jpg\",face)  #crops the detected face region from from frame and saves it as frame.jpg\n",
    "        #ab load krna\n",
    "        face=image.load_img(\"face.jpg\",target_size=(150,150))  #we r loading the images of faces\n",
    "        #now convert image into numpy array bcoz model ko dena h\n",
    "        face=image.img_to_array(face)  #python imaging lib into numpy array me convert\n",
    "        #now expand the dimension af the array\n",
    "        face=np.expand_dims(face,axis=0)  #0 staring me add krdega 150x150  cnn model accepts your data in 4d,first dimension no is considered as the no of sampes i.e 1\n",
    "        #prediction\n",
    "        out=cnn.predict(face)  #prediction hogo 0 and 1 me,with mask=0 bcz aphabetical order\n",
    "        if out>0.5:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)  #bgr format thus red  rectangle bnana in image\n",
    "            cv2.putText(img,'without mask',(x,y), 1, 4,(0,0,255))\n",
    "        else:\n",
    "            \n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            cv2.putText(img,'with mask',(x,y), 1, 4,(0,255,0))\n",
    "        \n",
    "    if cv2.waitKey(1)==97:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cap.DestroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373fff8-570c-4443-bf4a-7d4d53e04997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framee---> model\n",
    "#we hav to extract face frpm frame(just send face only)\n",
    "#perform preprocessing\n",
    "#send img to cnn model\n",
    "#detection of images k liy ek technology-->haarcascade\n",
    "#haarcascade-->this tech takes a lot of images of the target and store their haar features\n",
    "#result will be xml file where these are stored\n",
    "#face detection job will be done by haar cascade for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cc96b-d49e-4b22-8ef9-7c2dcd953c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()   #is se camera band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f6ba4-d3b7-43c6-821e-765cb21d024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ab image k aroound rectangle bnana"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
